<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<!-- Generated by javadoc (version 1.7.0_55) on Mon Jun 16 12:14:50 PDT 2014 -->
<title>SchemaRDD (Spark 1.1.0 JavaDoc)</title>
<meta name="date" content="2014-06-16">
<link rel="stylesheet" type="text/css" href="../../../../stylesheet.css" title="Style">
</head>
<body>
<script type="text/javascript"><!--
    if (location.href.indexOf('is-external=true') == -1) {
        parent.document.title="SchemaRDD (Spark 1.1.0 JavaDoc)";
    }
//-->
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar_top">
<!--   -->
</a><a href="#skip-navbar_top" title="Skip navigation links"></a><a name="navbar_top_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-all.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/apache/spark/sql/ScalaReflectionRelationSuite.html" title="class in org.apache.spark.sql"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../org/apache/spark/sql/SQLConf.html" title="interface in org.apache.spark.sql"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/apache/spark/sql/SchemaRDD.html" target="_top">Frames</a></li>
<li><a href="SchemaRDD.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="subTitle">org.apache.spark.sql</div>
<h2 title="Class SchemaRDD" class="title">Class SchemaRDD</h2>
</div>
<div class="contentContainer">
<ul class="inheritance">
<li>Object</li>
<li>
<ul class="inheritance">
<li><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">org.apache.spark.rdd.RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</li>
<li>
<ul class="inheritance">
<li>org.apache.spark.sql.SchemaRDD</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="description">
<ul class="blockList">
<li class="blockList">
<dl>
<dt>All Implemented Interfaces:</dt>
<dd>java.io.Serializable, <a href="../../../../org/apache/spark/Logging.html" title="interface in org.apache.spark">Logging</a></dd>
</dl>
<hr>
<br>
<pre>public class <span class="strong">SchemaRDD</span>
extends <a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</pre>
<div class="block">:: AlphaComponent ::
 An RDD of <code>Row</code> objects that has an associated schema. In addition to standard RDD functions,
 SchemaRDDs can be used in relational queries, as shown in the examples below.
 <p>
 Importing a SQLContext brings an implicit into scope that automatically converts a standard RDD
 whose elements are scala case classes into a SchemaRDD.  This conversion can also be done
 explicitly using the <code>createSchemaRDD</code> function on a <a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql"><code>SQLContext</code></a>.
 <p>
 A <code>SchemaRDD</code> can also be created by loading data in from external sources.
 Examples are loading data from Parquet files by using by using the
 <code>parquetFile</code> method on <a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql"><code>SQLContext</code></a>, and loading JSON datasets
 by using <code>jsonFile</code> and <code>jsonRDD</code> methods on <a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql"><code>SQLContext</code></a>.
 <p>
 == SQL Queries ==
 A SchemaRDD can be registered as a table in the <a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql"><code>SQLContext</code></a> that was used to create it.  Once
 an RDD has been registered as a table, it can be used in the FROM clause of SQL statements.
 <p>
 <pre><code>
  // One method for defining the schema of an RDD is to make a case class with the desired column
  // names and types.
  case class Record(key: Int, value: String)

  val sc: SparkContext // An existing spark context.
  val sqlContext = new SQLContext(sc)

  // Importing the SQL context gives access to all the SQL functions and implicit conversions.
  import sqlContext._

  val rdd = sc.parallelize((1 to 100).map(i =&gt; Record(i, s"val_$i")))
  // Any RDD containing case classes can be registered as a table.  The schema of the table is
  // automatically inferred using scala reflection.
  rdd.registerAsTable("records")

  val results: SchemaRDD = sql("SELECT * FROM records")
 </code></pre>
 <p>
 == Language Integrated Queries ==
 <p>
 <pre><code>

  case class Record(key: Int, value: String)

  val sc: SparkContext // An existing spark context.
  val sqlContext = new SQLContext(sc)

  // Importing the SQL context gives access to all the SQL functions and implicit conversions.
  import sqlContext._

  val rdd = sc.parallelize((1 to 100).map(i =&gt; Record(i, "val_" + i)))

  // Example of language integrated queries.
  rdd.where('key === 1).orderBy('value.asc).select('key).collect()
 </code></pre>
 <p></div>
<dl><dt><span class="strong">See Also:</span></dt><dd><a href="../../../../serialized-form.html#org.apache.spark.sql.SchemaRDD">Serialized Form</a></dd></dl>
</li>
</ul>
</div>
<div class="summary">
<ul class="blockList">
<li class="blockList">
<!-- ======== CONSTRUCTOR SUMMARY ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_summary">
<!--   -->
</a>
<h3>Constructor Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Constructor Summary table, listing constructors, and an explanation">
<caption><span>Constructors</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Constructor and Description</th>
</tr>
<tr class="altColor">
<td class="colOne"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#SchemaRDD(org.apache.spark.sql.SQLContext, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)">SchemaRDD</a></strong>(<a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</a>&nbsp;sqlContext,
         org.apache.spark.sql.catalyst.plans.logical.LogicalPlan&nbsp;baseLogicalPlan)</code>&nbsp;</td>
</tr>
</table>
</li>
</ul>
<!-- ========== METHOD SUMMARY =========== -->
<ul class="blockList">
<li class="blockList"><a name="method_summary">
<!--   -->
</a>
<h3>Method Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Method Summary table, listing methods, and an explanation">
<caption><span>Methods</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Method and Description</th>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#aggregate(scala.collection.Seq)">aggregate</a></strong>(scala.collection.Seq&lt;org.apache.spark.sql.catalyst.expressions.Expression&gt;&nbsp;aggregateExprs)</code>
<div class="block">Performs an aggregation over all Rows in this RDD.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#as(scala.Symbol)">as</a></strong>(scala.Symbol&nbsp;alias)</code>
<div class="block">Applies a qualifier to the attributes of this relation.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>org.apache.spark.sql.catalyst.plans.logical.LogicalPlan</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#baseLogicalPlan()">baseLogicalPlan</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#baseSchemaRDD()">baseSchemaRDD</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#coalesce(int, boolean, scala.math.Ordering)">coalesce</a></strong>(int&nbsp;numPartitions,
        boolean&nbsp;shuffle,
        scala.math.Ordering&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;ord)</code>
<div class="block">Return a new RDD that is reduced into <code>numPartitions</code> partitions.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>org.apache.spark.sql.catalyst.expressions.Row[]</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#collect()">collect</a></strong>()</code>
<div class="block">Return an array that contains all of the elements in this RDD.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>scala.collection.Iterator&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#compute(org.apache.spark.Partition, org.apache.spark.TaskContext)">compute</a></strong>(<a href="../../../../org/apache/spark/Partition.html" title="interface in org.apache.spark">Partition</a>&nbsp;split,
       <a href="../../../../org/apache/spark/TaskContext.html" title="class in org.apache.spark">TaskContext</a>&nbsp;context)</code>
<div class="block">:: DeveloperApi ::
 Implemented by subclasses to compute a given partition.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>long</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#count()">count</a></strong>()</code>
<div class="block">:: Experimental ::
 Return the number of elements in the RDD.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#distinct()">distinct</a></strong>()</code>
<div class="block">Return a new RDD containing the distinct elements in this RDD.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#distinct(int, scala.math.Ordering)">distinct</a></strong>(int&nbsp;numPartitions,
        scala.math.Ordering&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;ord)</code>
<div class="block">Return a new RDD containing the distinct elements in this RDD.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#filter(scala.Function1)">filter</a></strong>(scala.Function1&lt;org.apache.spark.sql.catalyst.expressions.Row,Object&gt;&nbsp;f)</code>
<div class="block">Return a new RDD containing only the elements that satisfy a predicate.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#generate(org.apache.spark.sql.catalyst.expressions.Generator, boolean, boolean, scala.Option)">generate</a></strong>(org.apache.spark.sql.catalyst.expressions.Generator&nbsp;generator,
        boolean&nbsp;join,
        boolean&nbsp;outer,
        scala.Option&lt;String&gt;&nbsp;alias)</code>
<div class="block">:: Experimental ::
 Applies the given Generator, or table generating function, to this relation.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/Partition.html" title="interface in org.apache.spark">Partition</a>[]</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#getPartitions()">getPartitions</a></strong>()</code>
<div class="block">Implemented by subclasses to return the set of partitions in this RDD.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#getSchemaTreeString()">getSchemaTreeString</a></strong>()</code>
<div class="block">Returns the output schema in the tree format.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#groupBy(scala.collection.Seq, scala.collection.Seq)">groupBy</a></strong>(scala.collection.Seq&lt;org.apache.spark.sql.catalyst.expressions.Expression&gt;&nbsp;groupingExprs,
       scala.collection.Seq&lt;org.apache.spark.sql.catalyst.expressions.Expression&gt;&nbsp;aggregateExprs)</code>
<div class="block">Performs a grouping followed by an aggregation.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#insertInto(java.lang.String)">insertInto</a></strong>(String&nbsp;tableName)</code>
<div class="block">:: Experimental ::
 Appends the rows from this RDD to the specified table.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#insertInto(java.lang.String, boolean)">insertInto</a></strong>(String&nbsp;tableName,
          boolean&nbsp;overwrite)</code>
<div class="block">:: Experimental ::
 Adds the rows from this RDD to the specified table, optionally overwriting the existing data.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#intersection(org.apache.spark.rdd.RDD)">intersection</a></strong>(<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;other)</code>
<div class="block">Return the intersection of this RDD and another one.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#intersection(org.apache.spark.rdd.RDD, int)">intersection</a></strong>(<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;other,
            int&nbsp;numPartitions)</code>
<div class="block">Return the intersection of this RDD and another one.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#intersection(org.apache.spark.rdd.RDD, org.apache.spark.Partitioner, scala.math.Ordering)">intersection</a></strong>(<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;other,
            <a href="../../../../org/apache/spark/Partitioner.html" title="class in org.apache.spark">Partitioner</a>&nbsp;partitioner,
            scala.math.Ordering&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;ord)</code>
<div class="block">Return the intersection of this RDD and another one.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#join(org.apache.spark.sql.SchemaRDD, org.apache.spark.sql.catalyst.plans.JoinType, scala.Option)">join</a></strong>(<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;otherPlan,
    org.apache.spark.sql.catalyst.plans.JoinType&nbsp;joinType,
    scala.Option&lt;org.apache.spark.sql.catalyst.expressions.Expression&gt;&nbsp;on)</code>
<div class="block">Performs a relational join on two SchemaRDDs</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#limit(org.apache.spark.sql.catalyst.expressions.Expression)">limit</a></strong>(org.apache.spark.sql.catalyst.expressions.Expression&nbsp;limitExpr)</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#limit(int)">limit</a></strong>(int&nbsp;limitNum)</code>
<div class="block">Limits the results by the given integer.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>org.apache.spark.sql.catalyst.plans.logical.LogicalPlan</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#logicalPlan()">logicalPlan</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#orderBy(scala.collection.Seq)">orderBy</a></strong>(scala.collection.Seq&lt;org.apache.spark.sql.catalyst.expressions.SortOrder&gt;&nbsp;sortExprs)</code>
<div class="block">Sorts the results by the given expressions.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#printSchema()">printSchema</a></strong>()</code>
<div class="block">Prints out the schema in the tree format.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>org.apache.spark.sql.SQLContext.QueryExecution</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#queryExecution()">queryExecution</a></strong>()</code>
<div class="block">:: DeveloperApi ::
 A lazily computed query execution workflow.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#registerAsTable(java.lang.String)">registerAsTable</a></strong>(String&nbsp;tableName)</code>
<div class="block">Registers this RDD as a temporary table using the given name.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#repartition(int, scala.math.Ordering)">repartition</a></strong>(int&nbsp;numPartitions,
           scala.math.Ordering&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;ord)</code>
<div class="block">Return a new RDD that has exactly numPartitions partitions.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#sample(boolean, double, long)">sample</a></strong>(boolean&nbsp;withReplacement,
      double&nbsp;fraction,
      long&nbsp;seed)</code>
<div class="block">:: Experimental ::
 Returns a sampled version of the underlying dataset.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#saveAsParquetFile(java.lang.String)">saveAsParquetFile</a></strong>(String&nbsp;path)</code>
<div class="block">Saves the contents of this <code>SchemaRDD</code> as a parquet file, preserving the schema.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#saveAsTable(java.lang.String)">saveAsTable</a></strong>(String&nbsp;tableName)</code>
<div class="block">:: Experimental ::
 Creates a table from the the contents of this SchemaRDD.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#select(scala.collection.Seq)">select</a></strong>(scala.collection.Seq&lt;org.apache.spark.sql.catalyst.expressions.NamedExpression&gt;&nbsp;exprs)</code>
<div class="block">Changes the output of this relation to the given expressions, similar to the <code>SELECT</code> clause
 in SQL.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#sqlContext()">sqlContext</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#subtract(org.apache.spark.rdd.RDD)">subtract</a></strong>(<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;other)</code>
<div class="block">Return an RDD with the elements from <code>this</code> that are not in <code>other</code>.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#subtract(org.apache.spark.rdd.RDD, int)">subtract</a></strong>(<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;other,
        int&nbsp;numPartitions)</code>
<div class="block">Return an RDD with the elements from <code>this</code> that are not in <code>other</code>.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#subtract(org.apache.spark.rdd.RDD, org.apache.spark.Partitioner, scala.math.Ordering)">subtract</a></strong>(<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;other,
        <a href="../../../../org/apache/spark/Partitioner.html" title="class in org.apache.spark">Partitioner</a>&nbsp;p,
        scala.math.Ordering&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;ord)</code>
<div class="block">Return an RDD with the elements from <code>this</code> that are not in <code>other</code>.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>org.apache.spark.sql.catalyst.expressions.Row[]</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#take(int)">take</a></strong>(int&nbsp;num)</code>
<div class="block">Take the first num elements of the RDD.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/api/java/JavaSchemaRDD.html" title="class in org.apache.spark.sql.api.java">JavaSchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#toJavaSchemaRDD()">toJavaSchemaRDD</a></strong>()</code>
<div class="block">Returns this RDD as a JavaSchemaRDD.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#toSchemaRDD()">toSchemaRDD</a></strong>()</code>
<div class="block">Returns this RDD as a SchemaRDD.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#toString()">toString</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#unionAll(org.apache.spark.sql.SchemaRDD)">unionAll</a></strong>(<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;otherPlan)</code>
<div class="block">Combines the tuples of two RDDs with the same schema, keeping duplicates.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#where(org.apache.spark.sql.catalyst.expressions.Expression)">where</a></strong>(org.apache.spark.sql.catalyst.expressions.Expression&nbsp;condition)</code>
<div class="block">Filters the output, only returning those rows where <code>condition</code> evaluates to true.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#where(scala.Function1)">where</a></strong>(scala.Function1&lt;org.apache.spark.sql.catalyst.expressions.DynamicRow,Object&gt;&nbsp;dynamicUdf)</code>
<div class="block">:: Experimental ::
 Filters tuples using a function over a <code>Dynamic</code> version of a given Row.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>&lt;T1&gt;&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/SchemaRDD.html#where(scala.Symbol, scala.Function1)">where</a></strong>(scala.Symbol&nbsp;arg1,
     scala.Function1&lt;T1,Object&gt;&nbsp;udf)</code>
<div class="block">Filters tuples using a function over the value of the specified column.</div>
</td>
</tr>
</table>
<ul class="blockList">
<li class="blockList"><a name="methods_inherited_from_class_org.apache.spark.rdd.RDD">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;org.apache.spark.rdd.<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a></h3>
<code><a href="../../../../org/apache/spark/rdd/RDD.html#aggregate(U, scala.Function2, scala.Function2, scala.reflect.ClassTag)">aggregate</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#cache()">cache</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#cartesian(org.apache.spark.rdd.RDD, scala.reflect.ClassTag)">cartesian</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#checkpoint()">checkpoint</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#checkpointData()">checkpointData</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#collect(scala.PartialFunction, scala.reflect.ClassTag)">collect</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#context()">context</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#countApprox(long, double)">countApprox</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#countApproxDistinct(double)">countApproxDistinct</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#countApproxDistinct(int, int)">countApproxDistinct</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#countByValue(scala.math.Ordering)">countByValue</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#countByValueApprox(long, double, scala.math.Ordering)">countByValueApprox</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#creationSiteInfo()">creationSiteInfo</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#dependencies()">dependencies</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#filterWith(scala.Function1, scala.Function2)">filterWith</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#first()">first</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#flatMap(scala.Function1, scala.reflect.ClassTag)">flatMap</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#flatMapWith(scala.Function1, boolean, scala.Function2, scala.reflect.ClassTag)">flatMapWith</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#fold(T, scala.Function2)">fold</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#foreach(scala.Function1)">foreach</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#foreachPartition(scala.Function1)">foreachPartition</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#foreachWith(scala.Function1, scala.Function2)">foreachWith</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#getCheckpointFile()">getCheckpointFile</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#getStorageLevel()">getStorageLevel</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#glom()">glom</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#groupBy(scala.Function1, scala.reflect.ClassTag)">groupBy</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#groupBy(scala.Function1, int, scala.reflect.ClassTag)">groupBy</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#groupBy(scala.Function1, org.apache.spark.Partitioner, scala.reflect.ClassTag, scala.math.Ordering)">groupBy</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#id()">id</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#isCheckpointed()">isCheckpointed</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#iterator(org.apache.spark.Partition, org.apache.spark.TaskContext)">iterator</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#keyBy(scala.Function1)">keyBy</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#map(scala.Function1, scala.reflect.ClassTag)">map</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#mapPartitions(scala.Function1, boolean, scala.reflect.ClassTag)">mapPartitions</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#mapPartitionsWithContext(scala.Function2, boolean, scala.reflect.ClassTag)">mapPartitionsWithContext</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#mapPartitionsWithIndex(scala.Function2, boolean, scala.reflect.ClassTag)">mapPartitionsWithIndex</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#mapPartitionsWithSplit(scala.Function2, boolean, scala.reflect.ClassTag)">mapPartitionsWithSplit</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#mapWith(scala.Function1, boolean, scala.Function2, scala.reflect.ClassTag)">mapWith</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#max(scala.math.Ordering)">max</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#min(scala.math.Ordering)">min</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#name()">name</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#partitioner()">partitioner</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#partitions()">partitions</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#persist()">persist</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#persist(org.apache.spark.storage.StorageLevel)">persist</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#pipe(scala.collection.Seq, scala.collection.Map, scala.Function1, scala.Function2, boolean)">pipe</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#pipe(java.lang.String)">pipe</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#pipe(java.lang.String, scala.collection.Map)">pipe</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#preferredLocations(org.apache.spark.Partition)">preferredLocations</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#randomSplit(double[], long)">randomSplit</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#reduce(scala.Function2)">reduce</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#saveAsObjectFile(java.lang.String)">saveAsObjectFile</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#saveAsTextFile(java.lang.String)">saveAsTextFile</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#saveAsTextFile(java.lang.String, java.lang.Class)">saveAsTextFile</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#setName(java.lang.String)">setName</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#sparkContext()">sparkContext</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#takeOrdered(int, scala.math.Ordering)">takeOrdered</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#takeSample(boolean, int, long)">takeSample</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#toArray()">toArray</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#toDebugString()">toDebugString</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#toJavaRDD()">toJavaRDD</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#toLocalIterator()">toLocalIterator</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#top(int, scala.math.Ordering)">top</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#toString()">toString</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#union(org.apache.spark.rdd.RDD)">union</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#unpersist(boolean)">unpersist</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#zip(org.apache.spark.rdd.RDD, scala.reflect.ClassTag)">zip</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#zipPartitions(org.apache.spark.rdd.RDD, boolean, scala.Function2, scala.reflect.ClassTag, scala.reflect.ClassTag)">zipPartitions</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#zipPartitions(org.apache.spark.rdd.RDD, scala.Function2, scala.reflect.ClassTag, scala.reflect.ClassTag)">zipPartitions</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#zipPartitions(org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, boolean, scala.Function3, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">zipPartitions</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#zipPartitions(org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, scala.Function3, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">zipPartitions</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#zipPartitions(org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, boolean, scala.Function4, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">zipPartitions</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#zipPartitions(org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, scala.Function4, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">zipPartitions</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#zipWithIndex()">zipWithIndex</a>, <a href="../../../../org/apache/spark/rdd/RDD.html#zipWithUniqueId()">zipWithUniqueId</a></code></li>
</ul>
<ul class="blockList">
<li class="blockList"><a name="methods_inherited_from_class_Object">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;Object</h3>
<code>equals, getClass, hashCode, notify, notifyAll, wait, wait, wait</code></li>
</ul>
<ul class="blockList">
<li class="blockList"><a name="methods_inherited_from_class_org.apache.spark.Logging">
<!--   -->
</a>
<h3>Methods inherited from interface&nbsp;org.apache.spark.<a href="../../../../org/apache/spark/Logging.html" title="interface in org.apache.spark">Logging</a></h3>
<code><a href="../../../../org/apache/spark/Logging.html#initialized()">initialized</a>, <a href="../../../../org/apache/spark/Logging.html#initializeIfNecessary()">initializeIfNecessary</a>, <a href="../../../../org/apache/spark/Logging.html#initializeLogging()">initializeLogging</a>, <a href="../../../../org/apache/spark/Logging.html#initLock()">initLock</a>, <a href="../../../../org/apache/spark/Logging.html#isTraceEnabled()">isTraceEnabled</a>, <a href="../../../../org/apache/spark/Logging.html#log_()">log_</a>, <a href="../../../../org/apache/spark/Logging.html#log()">log</a>, <a href="../../../../org/apache/spark/Logging.html#logDebug(scala.Function0)">logDebug</a>, <a href="../../../../org/apache/spark/Logging.html#logDebug(scala.Function0, java.lang.Throwable)">logDebug</a>, <a href="../../../../org/apache/spark/Logging.html#logError(scala.Function0)">logError</a>, <a href="../../../../org/apache/spark/Logging.html#logError(scala.Function0, java.lang.Throwable)">logError</a>, <a href="../../../../org/apache/spark/Logging.html#logInfo(scala.Function0)">logInfo</a>, <a href="../../../../org/apache/spark/Logging.html#logInfo(scala.Function0, java.lang.Throwable)">logInfo</a>, <a href="../../../../org/apache/spark/Logging.html#logTrace(scala.Function0)">logTrace</a>, <a href="../../../../org/apache/spark/Logging.html#logTrace(scala.Function0, java.lang.Throwable)">logTrace</a>, <a href="../../../../org/apache/spark/Logging.html#logWarning(scala.Function0)">logWarning</a>, <a href="../../../../org/apache/spark/Logging.html#logWarning(scala.Function0, java.lang.Throwable)">logWarning</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="details">
<ul class="blockList">
<li class="blockList">
<!-- ========= CONSTRUCTOR DETAIL ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_detail">
<!--   -->
</a>
<h3>Constructor Detail</h3>
<a name="SchemaRDD(org.apache.spark.sql.SQLContext, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>SchemaRDD</h4>
<pre>public&nbsp;SchemaRDD(<a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</a>&nbsp;sqlContext,
         org.apache.spark.sql.catalyst.plans.logical.LogicalPlan&nbsp;baseLogicalPlan)</pre>
</li>
</ul>
</li>
</ul>
<!-- ============ METHOD DETAIL ========== -->
<ul class="blockList">
<li class="blockList"><a name="method_detail">
<!--   -->
</a>
<h3>Method Detail</h3>
<a name="sqlContext()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sqlContext</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</a>&nbsp;sqlContext()</pre>
</li>
</ul>
<a name="baseLogicalPlan()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>baseLogicalPlan</h4>
<pre>public&nbsp;org.apache.spark.sql.catalyst.plans.logical.LogicalPlan&nbsp;baseLogicalPlan()</pre>
</li>
</ul>
<a name="baseSchemaRDD()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>baseSchemaRDD</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;baseSchemaRDD()</pre>
</li>
</ul>
<a name="compute(org.apache.spark.Partition, org.apache.spark.TaskContext)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>compute</h4>
<pre>public&nbsp;scala.collection.Iterator&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;compute(<a href="../../../../org/apache/spark/Partition.html" title="interface in org.apache.spark">Partition</a>&nbsp;split,
                                                                               <a href="../../../../org/apache/spark/TaskContext.html" title="class in org.apache.spark">TaskContext</a>&nbsp;context)</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#compute(org.apache.spark.Partition, org.apache.spark.TaskContext)">RDD</a></code></strong></div>
<div class="block">:: DeveloperApi ::
 Implemented by subclasses to compute a given partition.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#compute(org.apache.spark.Partition, org.apache.spark.TaskContext)">compute</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="getPartitions()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getPartitions</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/Partition.html" title="interface in org.apache.spark">Partition</a>[]&nbsp;getPartitions()</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#getPartitions()">RDD</a></code></strong></div>
<div class="block">Implemented by subclasses to return the set of partitions in this RDD. This method will only
 be called once, so it is safe to implement a time-consuming computation in it.</div>
</li>
</ul>
<a name="select(scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>select</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;select(scala.collection.Seq&lt;org.apache.spark.sql.catalyst.expressions.NamedExpression&gt;&nbsp;exprs)</pre>
<div class="block">Changes the output of this relation to the given expressions, similar to the <code>SELECT</code> clause
 in SQL.
 <p>
 <pre><code>
   schemaRDD.select('a, 'b + 'c, 'd as 'aliasedName)
 </code></pre>
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>exprs</code> - a set of logical expression that will be evaluated for each input row.
 <p></dd></dl>
</li>
</ul>
<a name="where(org.apache.spark.sql.catalyst.expressions.Expression)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>where</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;where(org.apache.spark.sql.catalyst.expressions.Expression&nbsp;condition)</pre>
<div class="block">Filters the output, only returning those rows where <code>condition</code> evaluates to true.
 <p>
 <pre><code>
   schemaRDD.where('a === 'b)
   schemaRDD.where('a === 1)
   schemaRDD.where('a + 'b &gt; 10)
 </code></pre>
 <p></div>
</li>
</ul>
<a name="join(org.apache.spark.sql.SchemaRDD, org.apache.spark.sql.catalyst.plans.JoinType, scala.Option)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>join</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;join(<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;otherPlan,
             org.apache.spark.sql.catalyst.plans.JoinType&nbsp;joinType,
             scala.Option&lt;org.apache.spark.sql.catalyst.expressions.Expression&gt;&nbsp;on)</pre>
<div class="block">Performs a relational join on two SchemaRDDs
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>otherPlan</code> - the <a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql"><code>SchemaRDD</code></a> that should be joined with this one.</dd><dd><code>joinType</code> - One of <code>Inner</code>, <code>LeftOuter</code>, <code>RightOuter</code>, or <code>FullOuter</code>. Defaults to <code>Inner.</code></dd><dd><code>on</code> - An optional condition for the join operation.  This is equivalent to the <code>ON</code>
                 clause in standard SQL.  In the case of <code>Inner</code> joins, specifying a
                 <code>condition</code> is equivalent to adding <code>where</code> clauses after the <code>join</code>.
 <p></dd></dl>
</li>
</ul>
<a name="orderBy(scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>orderBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;orderBy(scala.collection.Seq&lt;org.apache.spark.sql.catalyst.expressions.SortOrder&gt;&nbsp;sortExprs)</pre>
<div class="block">Sorts the results by the given expressions.
 <pre><code>
   schemaRDD.orderBy('a)
   schemaRDD.orderBy('a, 'b)
   schemaRDD.orderBy('a.asc, 'b.desc)
 </code></pre>
 <p></div>
</li>
</ul>
<a name="limit(org.apache.spark.sql.catalyst.expressions.Expression)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>limit</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;limit(org.apache.spark.sql.catalyst.expressions.Expression&nbsp;limitExpr)</pre>
</li>
</ul>
<a name="limit(int)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>limit</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;limit(int&nbsp;limitNum)</pre>
<div class="block">Limits the results by the given integer.
 <pre><code>
   schemaRDD.limit(10)
 </code></pre></div>
</li>
</ul>
<a name="groupBy(scala.collection.Seq, scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>groupBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;groupBy(scala.collection.Seq&lt;org.apache.spark.sql.catalyst.expressions.Expression&gt;&nbsp;groupingExprs,
                scala.collection.Seq&lt;org.apache.spark.sql.catalyst.expressions.Expression&gt;&nbsp;aggregateExprs)</pre>
<div class="block">Performs a grouping followed by an aggregation.
 <p>
 <pre><code>
   schemaRDD.groupBy('year)(Sum('sales) as 'totalSales)
 </code></pre>
 <p></div>
</li>
</ul>
<a name="aggregate(scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>aggregate</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;aggregate(scala.collection.Seq&lt;org.apache.spark.sql.catalyst.expressions.Expression&gt;&nbsp;aggregateExprs)</pre>
<div class="block">Performs an aggregation over all Rows in this RDD.
 This is equivalent to a groupBy with no grouping expressions.
 <p>
 <pre><code>
   schemaRDD.aggregate(Sum('sales) as 'totalSales)
 </code></pre>
 <p></div>
</li>
</ul>
<a name="as(scala.Symbol)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>as</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;as(scala.Symbol&nbsp;alias)</pre>
<div class="block">Applies a qualifier to the attributes of this relation.  Can be used to disambiguate attributes
 with the same name, for example, when performing self-joins.
 <p>
 <pre><code>
   val x = schemaRDD.where('a === 1).as('x)
   val y = schemaRDD.where('a === 2).as('y)
   x.join(y).where("x.a".attr === "y.a".attr),
 </code></pre>
 <p></div>
</li>
</ul>
<a name="unionAll(org.apache.spark.sql.SchemaRDD)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>unionAll</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;unionAll(<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;otherPlan)</pre>
<div class="block">Combines the tuples of two RDDs with the same schema, keeping duplicates.
 <p></div>
</li>
</ul>
<a name="where(scala.Symbol, scala.Function1)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>where</h4>
<pre>public&nbsp;&lt;T1&gt;&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;where(scala.Symbol&nbsp;arg1,
                   scala.Function1&lt;T1,Object&gt;&nbsp;udf)</pre>
<div class="block">Filters tuples using a function over the value of the specified column.
 <p>
 <pre><code>
   schemaRDD.sfilter('a)((a: Int) =&gt; ...)
 </code></pre>
 <p></div>
</li>
</ul>
<a name="where(scala.Function1)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>where</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;where(scala.Function1&lt;org.apache.spark.sql.catalyst.expressions.DynamicRow,Object&gt;&nbsp;dynamicUdf)</pre>
<div class="block">:: Experimental ::
 Filters tuples using a function over a <code>Dynamic</code> version of a given Row.  DynamicRows use
 scala's Dynamic trait to emulate an ORM of in a dynamically typed language.  Since the type of
 the column is not known at compile time, all attributes are converted to strings before
 being passed to the function.
 <p>
 <pre><code>
   schemaRDD.where(r =&gt; r.firstName == "Bob" && r.lastName == "Smith")
 </code></pre>
 <p></div>
</li>
</ul>
<a name="sample(boolean, double, long)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sample</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;sample(boolean&nbsp;withReplacement,
               double&nbsp;fraction,
               long&nbsp;seed)</pre>
<div class="block">:: Experimental ::
 Returns a sampled version of the underlying dataset.
 <p></div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#sample(boolean, double, long)">sample</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="count()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>count</h4>
<pre>public&nbsp;long&nbsp;count()</pre>
<div class="block">:: Experimental ::
 Return the number of elements in the RDD. Unlike the base RDD implementation of count, this
 implementation leverages the query optimizer to compute the count on the SchemaRDD, which
 supports features such as filter pushdown.</div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#count()">count</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="generate(org.apache.spark.sql.catalyst.expressions.Generator, boolean, boolean, scala.Option)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>generate</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;generate(org.apache.spark.sql.catalyst.expressions.Generator&nbsp;generator,
                 boolean&nbsp;join,
                 boolean&nbsp;outer,
                 scala.Option&lt;String&gt;&nbsp;alias)</pre>
<div class="block">:: Experimental ::
 Applies the given Generator, or table generating function, to this relation.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>generator</code> - A table generating function.  The API for such functions is likely to change
                  in future releases</dd><dd><code>join</code> - when set to true, each output row of the generator is joined with the input row
             that produced it.</dd><dd><code>outer</code> - when set to true, at least one row will be produced for each input row, similar to
              an <code>OUTER JOIN</code> in SQL.  When no output rows are produced by the generator for a
              given row, a single row will be output, with <code>NULL</code> values for each of the
              generated columns.</dd><dd><code>alias</code> - an optional alias that can be used as qualifier for the attributes that are
              produced by this generate operation.
 <p></dd></dl>
</li>
</ul>
<a name="toSchemaRDD()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>toSchemaRDD</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;toSchemaRDD()</pre>
<div class="block">Returns this RDD as a SchemaRDD.  Intended primarily to force the invocation of the implicit
 conversion from a standard RDD to a SchemaRDD.
 <p></div>
</li>
</ul>
<a name="toJavaSchemaRDD()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>toJavaSchemaRDD</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/api/java/JavaSchemaRDD.html" title="class in org.apache.spark.sql.api.java">JavaSchemaRDD</a>&nbsp;toJavaSchemaRDD()</pre>
<div class="block">Returns this RDD as a JavaSchemaRDD.
 <p></div>
</li>
</ul>
<a name="collect()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>collect</h4>
<pre>public&nbsp;org.apache.spark.sql.catalyst.expressions.Row[]&nbsp;collect()</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#collect()">RDD</a></code></strong></div>
<div class="block">Return an array that contains all of the elements in this RDD.</div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#collect()">collect</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="take(int)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>take</h4>
<pre>public&nbsp;org.apache.spark.sql.catalyst.expressions.Row[]&nbsp;take(int&nbsp;num)</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#take(int)">RDD</a></code></strong></div>
<div class="block">Take the first num elements of the RDD. It works by first scanning one partition, and use the
 results from that partition to estimate the number of additional partitions needed to satisfy
 the limit.</div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#take(int)">take</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="coalesce(int, boolean, scala.math.Ordering)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>coalesce</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;coalesce(int&nbsp;numPartitions,
                 boolean&nbsp;shuffle,
                 scala.math.Ordering&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;ord)</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#coalesce(int, boolean, scala.math.Ordering)">RDD</a></code></strong></div>
<div class="block">Return a new RDD that is reduced into <code>numPartitions</code> partitions.
 <p>
 This results in a narrow dependency, e.g. if you go from 1000 partitions
 to 100 partitions, there will not be a shuffle, instead each of the 100
 new partitions will claim 10 of the current partitions.
 <p>
 However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
 this may result in your computation taking place on fewer nodes than
 you like (e.g. one node in the case of numPartitions = 1). To avoid this,
 you can pass shuffle = true. This will add a shuffle step, but means the
 current upstream partitions will be executed in parallel (per whatever
 the current partitioning is).
 <p>
 Note: With shuffle = true, you can actually coalesce to a larger number
 of partitions. This is useful if you have a small number of partitions,
 say 100, potentially with a few partitions being abnormally large. Calling
 coalesce(1000, shuffle = true) will result in 1000 partitions with the
 data distributed using a hash partitioner.</div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#coalesce(int, boolean, scala.math.Ordering)">coalesce</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="distinct()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>distinct</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;distinct()</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#distinct()">RDD</a></code></strong></div>
<div class="block">Return a new RDD containing the distinct elements in this RDD.</div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#distinct()">distinct</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="distinct(int, scala.math.Ordering)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>distinct</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;distinct(int&nbsp;numPartitions,
                 scala.math.Ordering&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;ord)</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#distinct(int, scala.math.Ordering)">RDD</a></code></strong></div>
<div class="block">Return a new RDD containing the distinct elements in this RDD.</div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#distinct(int, scala.math.Ordering)">distinct</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="filter(scala.Function1)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>filter</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;filter(scala.Function1&lt;org.apache.spark.sql.catalyst.expressions.Row,Object&gt;&nbsp;f)</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#filter(scala.Function1)">RDD</a></code></strong></div>
<div class="block">Return a new RDD containing only the elements that satisfy a predicate.</div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#filter(scala.Function1)">filter</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="intersection(org.apache.spark.rdd.RDD)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>intersection</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;intersection(<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;other)</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#intersection(org.apache.spark.rdd.RDD)">RDD</a></code></strong></div>
<div class="block">Return the intersection of this RDD and another one. The output will not contain any duplicate
 elements, even if the input RDDs did.
 <p>
 Note that this method performs a shuffle internally.</div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#intersection(org.apache.spark.rdd.RDD)">intersection</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="intersection(org.apache.spark.rdd.RDD, org.apache.spark.Partitioner, scala.math.Ordering)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>intersection</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;intersection(<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;other,
                     <a href="../../../../org/apache/spark/Partitioner.html" title="class in org.apache.spark">Partitioner</a>&nbsp;partitioner,
                     scala.math.Ordering&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;ord)</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#intersection(org.apache.spark.rdd.RDD, org.apache.spark.Partitioner, scala.math.Ordering)">RDD</a></code></strong></div>
<div class="block">Return the intersection of this RDD and another one. The output will not contain any duplicate
 elements, even if the input RDDs did.
 <p>
 Note that this method performs a shuffle internally.
 <p></div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#intersection(org.apache.spark.rdd.RDD, org.apache.spark.Partitioner, scala.math.Ordering)">intersection</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
<dd><code>partitioner</code> - Partitioner to use for the resulting RDD</dd></dl>
</li>
</ul>
<a name="intersection(org.apache.spark.rdd.RDD, int)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>intersection</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;intersection(<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;other,
                     int&nbsp;numPartitions)</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#intersection(org.apache.spark.rdd.RDD, int)">RDD</a></code></strong></div>
<div class="block">Return the intersection of this RDD and another one. The output will not contain any duplicate
 elements, even if the input RDDs did.  Performs a hash partition across the cluster
 <p>
 Note that this method performs a shuffle internally.
 <p></div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#intersection(org.apache.spark.rdd.RDD, int)">intersection</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
<dd><code>numPartitions</code> - How many partitions to use in the resulting RDD</dd></dl>
</li>
</ul>
<a name="repartition(int, scala.math.Ordering)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>repartition</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;repartition(int&nbsp;numPartitions,
                    scala.math.Ordering&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;ord)</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#repartition(int, scala.math.Ordering)">RDD</a></code></strong></div>
<div class="block">Return a new RDD that has exactly numPartitions partitions.
 <p>
 Can increase or decrease the level of parallelism in this RDD. Internally, this uses
 a shuffle to redistribute data.
 <p>
 If you are decreasing the number of partitions in this RDD, consider using <code>coalesce</code>,
 which can avoid performing a shuffle.</div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#repartition(int, scala.math.Ordering)">repartition</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="subtract(org.apache.spark.rdd.RDD)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>subtract</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;subtract(<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;other)</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#subtract(org.apache.spark.rdd.RDD)">RDD</a></code></strong></div>
<div class="block">Return an RDD with the elements from <code>this</code> that are not in <code>other</code>.
 <p>
 Uses <code>this</code> partitioner/partition size, because even if <code>other</code> is huge, the resulting
 RDD will be <= us.</div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#subtract(org.apache.spark.rdd.RDD)">subtract</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="subtract(org.apache.spark.rdd.RDD, int)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>subtract</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;subtract(<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;other,
                 int&nbsp;numPartitions)</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#subtract(org.apache.spark.rdd.RDD, int)">RDD</a></code></strong></div>
<div class="block">Return an RDD with the elements from <code>this</code> that are not in <code>other</code>.</div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#subtract(org.apache.spark.rdd.RDD, int)">subtract</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="subtract(org.apache.spark.rdd.RDD, org.apache.spark.Partitioner, scala.math.Ordering)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>subtract</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SchemaRDD.html" title="class in org.apache.spark.sql">SchemaRDD</a>&nbsp;subtract(<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;other,
                 <a href="../../../../org/apache/spark/Partitioner.html" title="class in org.apache.spark">Partitioner</a>&nbsp;p,
                 scala.math.Ordering&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;&nbsp;ord)</pre>
<div class="block"><strong>Description copied from class:&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html#subtract(org.apache.spark.rdd.RDD, org.apache.spark.Partitioner, scala.math.Ordering)">RDD</a></code></strong></div>
<div class="block">Return an RDD with the elements from <code>this</code> that are not in <code>other</code>.</div>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/rdd/RDD.html#subtract(org.apache.spark.rdd.RDD, org.apache.spark.Partitioner, scala.math.Ordering)">subtract</a></code>&nbsp;in class&nbsp;<code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.catalyst.expressions.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="queryExecution()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>queryExecution</h4>
<pre>public&nbsp;org.apache.spark.sql.SQLContext.QueryExecution&nbsp;queryExecution()</pre>
<div class="block">:: DeveloperApi ::
 A lazily computed query execution workflow.  All other RDD operations are passed
 through to the RDD that is produced by this workflow. This workflow is produced lazily because
 invoking the whole query optimization pipeline can be expensive.
 <p>
 The query execution is considered a Developer API as phases may be added or removed in future
 releases.  This execution is only exposed to provide an interface for inspecting the various
 phases for debugging purposes.  Applications should not depend on particular phases existing
 or producing any specific output, even for exactly the same query.
 <p>
 Additionally, the RDD exposed by this execution is not designed for consumption by end users.
 In particular, it does not contain any schema information, and it reuses Row objects
 internally.  This object reuse improves performance, but can make programming against the RDD
 more difficult.  Instead end users should perform RDD operations on a SchemaRDD directly.</div>
</li>
</ul>
<a name="logicalPlan()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>logicalPlan</h4>
<pre>public&nbsp;org.apache.spark.sql.catalyst.plans.logical.LogicalPlan&nbsp;logicalPlan()</pre>
</li>
</ul>
<a name="toString()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>toString</h4>
<pre>public&nbsp;String&nbsp;toString()</pre>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code>toString</code>&nbsp;in class&nbsp;<code>Object</code></dd>
</dl>
</li>
</ul>
<a name="saveAsParquetFile(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>saveAsParquetFile</h4>
<pre>public&nbsp;void&nbsp;saveAsParquetFile(String&nbsp;path)</pre>
<div class="block">Saves the contents of this <code>SchemaRDD</code> as a parquet file, preserving the schema.  Files that
 are written out using this method can be read back in as a SchemaRDD using the <code>parquetFile</code>
 function.
 <p></div>
</li>
</ul>
<a name="registerAsTable(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>registerAsTable</h4>
<pre>public&nbsp;void&nbsp;registerAsTable(String&nbsp;tableName)</pre>
<div class="block">Registers this RDD as a temporary table using the given name.  The lifetime of this temporary
 table is tied to the <a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql"><code>SQLContext</code></a> that was used to create this SchemaRDD.
 <p></div>
</li>
</ul>
<a name="insertInto(java.lang.String, boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>insertInto</h4>
<pre>public&nbsp;void&nbsp;insertInto(String&nbsp;tableName,
              boolean&nbsp;overwrite)</pre>
<div class="block">:: Experimental ::
 Adds the rows from this RDD to the specified table, optionally overwriting the existing data.
 <p></div>
</li>
</ul>
<a name="insertInto(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>insertInto</h4>
<pre>public&nbsp;void&nbsp;insertInto(String&nbsp;tableName)</pre>
<div class="block">:: Experimental ::
 Appends the rows from this RDD to the specified table.
 <p></div>
</li>
</ul>
<a name="saveAsTable(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>saveAsTable</h4>
<pre>public&nbsp;void&nbsp;saveAsTable(String&nbsp;tableName)</pre>
<div class="block">:: Experimental ::
 Creates a table from the the contents of this SchemaRDD.  This will fail if the table already
 exists.
 <p>
 Note that this currently only works with SchemaRDDs that are created from a HiveContext as
 there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
 an RDD out to a parquet file, and then register that file as a table.  This "table" can then
 be the target of an <code>insertInto</code>.
 <p></div>
</li>
</ul>
<a name="getSchemaTreeString()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getSchemaTreeString</h4>
<pre>public&nbsp;String&nbsp;getSchemaTreeString()</pre>
<div class="block">Returns the output schema in the tree format.</div>
</li>
</ul>
<a name="printSchema()">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>printSchema</h4>
<pre>public&nbsp;void&nbsp;printSchema()</pre>
<div class="block">Prints out the schema in the tree format.</div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- ========= END OF CLASS DATA ========= -->
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar_bottom">
<!--   -->
</a><a href="#skip-navbar_bottom" title="Skip navigation links"></a><a name="navbar_bottom_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-all.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/apache/spark/sql/ScalaReflectionRelationSuite.html" title="class in org.apache.spark.sql"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../org/apache/spark/sql/SQLConf.html" title="interface in org.apache.spark.sql"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/apache/spark/sql/SchemaRDD.html" target="_top">Frames</a></li>
<li><a href="SchemaRDD.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
</body>
</html>
